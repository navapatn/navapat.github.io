---
permalink: /
title: "Welcome!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Navapat Nananukul or you can call me Pat. I am currently working as a student reseacher at USC Information Sciences Institute, under the advise of [Prof. Mayank Kejriwal](https://viterbi.usc.edu/directory/faculty/Kejriwal/Mayank). I am a junior researcher in the field of _Aritificial Intelligence_, I am motivated by its capacity to emulate human-like behaviors that we, as humans, train these machines. My research lies at the intersection of two complementary fields _Large Language models_ and _Knowledge Graph_, both of which are crucial for enabling machines to communicate, interact, and reason in ways that are remarkably human.

My reseach interest questions mainly include:

- How to build human intelligence structure to improve machine learning and language model i.e. Knowledge graph, ontology, etc.
- How to build a process that allow human to understand large language models.
- Application of knowledge graph to language models i.e. Question-answering, Commonsense-reasoning, etc.
- How to reduce false information, hallucination, bias in language model.



About me
======

I received my Master's degree from **USC Viterbi Department of Computer Science**. In the last semester, I was fortunate to be advised by [Prof. Mohammad Rostami](https://viterbi.usc.edu/directory/faculty/Rostami/Mohammad) who gave me the first official research experience in the CS field. We worked on improving domain adpatation, image segmentation performance for medical MRI scan images. I received my Bachelor's degree in Computer Science from Chulalongkorn University with a class honor.

Research
======

Recently, I have been working on topics that allow human to understand state-of-the-art LLMs and utilize them efficeintly. Specifically, **(1) Knowledge graph ontology design for LLMs hallucination** aimed to capture and categorize hallucination we see from prompts and answers from large language model, and systemetically store into a knowledge graph structure so that we can utilize this data and potentially improve or intervene prompting processes to reduce hallucination, bias, etc. Moreover, I started working on the topic of knowledge graph building and Large Language Models. The works include **(2) Blocks prioritization for unsupervised entity resolution in the era of LLMs** where we propose algorithm that bridge the gap between LLMs and one of the most important steps in knowledge graph building entity resolution (ER) while also considering cost and maintaining great ER performance. Lastly, **(3) Cost-aware prompt engineering in unsupervised entity resolution** aimed to gauge the performance of state-of-the-art LLMs in entity resolution tasks. We further analyze of our results and provide comprehensive analysis for each prompt method on entity resolution tasks.

My ongoing research list is available in this link: [Current Research](https://navapatn.github.io/talks/)


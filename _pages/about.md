---
permalink: /
title: "Welcome!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Navapat Nananukul or you can call me Pat. I am currently working as a student researcher at USC Information Sciences Institute under the advice of [Prof. Mayank Kejriwal](https://viterbi.usc.edu/directory/faculty/Kejriwal/Mayank). I am a junior researcher in the field of _a\Artificial intelligence_; I am motivated by its capacity to emulate human-like behaviors that we, as humans, train these machines. My research lies at the intersection of two complementary fields _Large Language Models_ and _Knowledge Graphs_, which are crucial for enabling machines to communicate, interact, and reason in remarkably human ways.

My reseach interest questions mainly include:

- How to build human intelligence structure to improve machine learning and language model i.e. Knowledge graph, ontology, etc.
- How to build a process that allow human to understand large language models.
- Application of knowledge graph to language models i.e. Question-answering, Commonsense-reasoning, etc.
- How to reduce false information, hallucination, bias in language model.



About me
======

I received my Master's degree from **USC Viterbi Department of Computer Science**. In the last semester, I was fortunate to be advised by [Prof. Mohammad Rostami](https://viterbi.usc.edu/directory/faculty/Rostami/Mohammad) who gave me the first official research experience in the CS field. We worked on improving domain adpatation, image segmentation performance for medical MRI scan images. I received my Bachelor's degree in Computer Science from Chulalongkorn University with a class honor.

Research
======

Recently, I have been working on topics that allow humans to understand and utilize state-of-the-art LLMs efficiently. Specifically, **(1) Knowledge graph ontology design for LLMs hallucination** aimed to capture and categorize hallucinations we see from prompts and answers from a large language model and systematically store into a knowledge graph structure so that we can utilize this data and potentially improve or intervene prompting processes to reduce hallucination, bias, etc. Moreover, I started working on knowledge graph building and large language models. The works include **(2) Blocks prioritization for unsupervised entity resolution in the era of LLMs**, where we propose an algorithm that bridges the gap between LLMs and one of the most essential steps in knowledge graph-building entity resolution (ER) while also considering cost and maintaining excellent ER performance. Lastly, **(3) Cost-aware prompt engineering in unsupervised entity resolution** aimed to gauge the performance of state-of-the-art LLMs in entity resolution tasks. We further analyze our results and comprehensively analyze each prompt method on entity resolution tasks.

My ongoing research list is available in this link: [Current Research](https://navapatn.github.io/talks/)

